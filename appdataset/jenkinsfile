pipeline {
    agent any
    stages {
        stage('Pre-requisites') {
            steps {
                script {
                    // Create a local directory for kubectl
                    sh '''
                    mkdir -p ~/.local/bin
                    curl -LO "https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl"
                    chmod +x ./kubectl
                    mv ./kubectl ~/.local/bin/kubectl
                    export PATH=$PATH:$HOME/.local/bin
                    kubectl config use-context minikube
                    pip3 install requests pandas scikit-learn joblib
                    '''
                    // Add the local bin to the PATH
                    env.PATH += ":$HOME/.local/bin"

                }
            }
        }
/*       stage('Run kubectl commands') {
            steps {
                script {
                    // Ensure the PATH is set for the current session
                    sh '''
                    export PATH=$PATH:$HOME/.local/bin
                    kubectl config use-context minikube
                    kubectl get nodes
                    kubectl get pods -A
                    pip3 install requests pandas scikit-learn joblib
                    '''
                }
            }
        }*/
        stage('Scrape metric and create CSV') {
            steps {
                script {
            def pythonScript = '''
import os
import time
import requests
import pandas as pd
from datetime import datetime

# Define Prometheus URL and Queries
PROMETHEUS_URL = 'http://localhost:9090/api/v1/query'
MEMORY_QUERY = 'avg_over_time(container_memory_usage_bytes{namespace="default",pod=~"hamster-.*"}[5m])'
CPU_QUERY = 'rate(container_cpu_usage_seconds_total{namespace="default",pod=~"hamster-.*"}[5m])'

def query_prometheus(query, start_time, end_time, step):
    """Query Prometheus for a time range to ensure both CPU and Memory are synchronized."""
    response = requests.get(PROMETHEUS_URL, params={
        'query': query,
        'start': start_time,
        'end': end_time,
        'step': step
    })
    return response.json()


def parse_prometheus_response(data):
    results = []
    if data['status'] == 'success':
        for result in data['data']['result']:
            pod_name = result['metric'].get('pod', 'unknown')
            value = result['value']
            timestamp = datetime.fromtimestamp(float(value[0])).isoformat() + 'Z'  # Format as ISO 8601
            usage = float(value[1])
            results.append((timestamp, pod_name, usage))
    return results

# Set the time range for the query (last 1 minute)
start_time = int(time.time()) - 60  # 1 minute ago
print("starttime:" + str(start_time))
end_time = int(time.time())         # Now
print("endtime:" + str(end_time))
step = 15  # Interval in seconds

# Query CPU data
cpu_data = query_prometheus(CPU_QUERY, start_time, end_time, step)
cpu_df = pd.DataFrame(parse_prometheus_response(cpu_data), columns=['timestamp', 'pod_name', 'cpu_usage'])

# Query Memory data
memory_data = query_prometheus(MEMORY_QUERY, start_time, end_time, step)
memory_df = pd.DataFrame(parse_prometheus_response(memory_data), columns=['timestamp', 'pod_name', 'memory_usage'])

# Round timestamps in both DataFrames to the nearest second
cpu_df['timestamp'] = pd.to_datetime(cpu_df['timestamp']).dt.round('s')
memory_df['timestamp'] = pd.to_datetime(memory_df['timestamp']).dt.round('s')

# Combine CPU and Memory DataFrames
combined_df = pd.merge(cpu_df, memory_df, on=['timestamp', 'pod_name'], how='outer')

# Fill NaN values with 0 or apply other strategies
combined_df['cpu_usage'].fillna(0, inplace=True)
combined_df['memory_usage'].fillna(0, inplace=True)

# Convert cpu_usage to milli units (m) and memory_usage to Mi
combined_df['cpu_usage'] = (combined_df['cpu_usage'] * 1000).astype(int).astype(str) + 'm'
combined_df['memory_usage'] = (combined_df['memory_usage'] / (1024 * 1024)).astype(int).astype(str) + 'Mi'

# Assuming target requests need to be 20% higher
combined_df['target_cpu_request'] = ((combined_df['cpu_usage'].str.replace('m', '').astype(int) * 1.2).round().astype(int).astype(str) + 'm')
combined_df['target_memory_request'] = ((combined_df['memory_usage'].str.replace('Mi', '').astype(int) * 1.2).round().astype(int).astype(str) + 'Mi')

# Reorder the columns to match the expected CSV format
combined_df = combined_df[['timestamp', 'pod_name', 'cpu_usage', 'memory_usage', 'target_cpu_request', 'target_memory_request']]

# Check if the CSV file already exists
file_path = 'hamster_k8s_resource_usage.csv'

if os.path.exists(file_path):
    # Append data to the existing CSV file
    combined_df.to_csv(file_path, mode='a', header=False, index=False)
else:
    # Create a new CSV file
    combined_df.to_csv(file_path, index=False)

print("Data has been saved to hamster_k8s_resource_usage.csv")
'''
            writeFile file: 'scrape_prometheus.py', text: pythonScript
            sh 'python3 scrape_prometheus.py'
        }
    }
}
        stage('Dataset copy to location') {
            steps {
                script {
                    // Ensure the PATH is set for the current session
                    sh '''
                    cp ~/.jenkins/workspace/app_dataset_csv/hamster_k8s_resource_usage.csv ~/Documents
                    '''
                }
            }
        }
    }
}
